{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import sklearn.preprocessing\n",
    "from torch.utils.data import Dataset\n",
    "root_dir=\"\"\n",
    "online=False\n",
    "import sklearn.preprocessing\n",
    "from torch.utils.data import Dataset\n",
    "if online:\n",
    "    root_dir=\"/kaggle/input/lish-moa/\"\n",
    "else:\n",
    "    root_dir=\"/data/lish-moa/\"\n",
    "\n",
    "n_train=23814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "class MoADataset(Dataset):\n",
    "    '''\n",
    "    Nice reference : https://pytorch.org/docs/stable/_modules/torchvision/datasets/mnist.html#MNIST\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, root_dir, form='train',transform=None,validation_split=20000):\n",
    "        #assert train #test not implemented yet!\n",
    "        self.form=form\n",
    "        if form=='train':\n",
    "            self.features=MoADataset.preprocess_features(pd.read_csv(root_dir+\"train_features.csv\").iloc[:validation_split])\n",
    "            self.targets=pd.read_csv(root_dir+\"train_targets_scored.csv\").iloc[:validation_split]\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.weights_dict={i:1 for i in range(len(self.targets))}\n",
    "        elif form=='validation':\n",
    "            self.features=MoADataset.preprocess_features(pd.read_csv(root_dir+\"train_features.csv\").iloc[validation_split:])\n",
    "            self.targets=pd.read_csv(root_dir+\"train_targets_scored.csv\").iloc[validation_split:]\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.weights_dict={i:1 for i in range(len(self.targets))}\n",
    "        else:\n",
    "            self.features=MoADataset.preprocess_features(pd.read_csv(root_dir+\"test_features.csv\"))\n",
    "            self.targets=None\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "            \n",
    "            \n",
    "        features=np.array(self.features.iloc[idx,1:]).astype(np.float32)\n",
    "        if self.transform:\n",
    "                features = self.transform(features)\n",
    "        if self.form!='test':\n",
    "            targets=np.array(self.targets.iloc[idx,1:]).astype(np.float32)\n",
    "\n",
    "            return features,targets,idx\n",
    "        else: \n",
    "            return features\n",
    "        \n",
    "        \n",
    "\n",
    "    def preprocess_features(df):\n",
    "        #raw_train_data[[col for col in raw_train_data.columns if col.startswith('g-')]]\n",
    "        #raw_train_data[[col for col in raw_train_data.columns if col.startswith('c-')]]\n",
    "        #raw_train_data[['cp_time']]\n",
    "        #raw_train_data[['cp_type','cp_dose']]\n",
    "\n",
    "        enc=sklearn.preprocessing.OrdinalEncoder()\n",
    "        enc.fit(df[['cp_type','cp_dose']])\n",
    "        df[['cp_type','cp_dose']]=enc.transform(df[['cp_type','cp_dose']])\n",
    "        return df\n",
    "    \n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "training_full_data=MoADataset(\"data/lish-moa/\",form='train',transform=None,validation_split=n_train)\n",
    "train_full_loader=torch.utils.data.DataLoader(training_full_data,batch_size=100,drop_last=True,)\n",
    "\n",
    "training_data=MoADataset(\"data/lish-moa/\",form='train',transform=None)\n",
    "train_loader=torch.utils.data.DataLoader(training_data,batch_size=100,drop_last=True)\n",
    "\n",
    "validation_data=MoADataset(\"data/lish-moa/\",form='validation',transform=None)\n",
    "validation_loader=torch.utils.data.DataLoader(validation_data,batch_size=len(validation_data),drop_last=True)\n",
    "\n",
    "test_data=MoADataset(\"data/lish-moa/\",form='test',transform=None)\n",
    "test_loader=torch.utils.data.DataLoader(test_data,batch_size=len(test_data))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code from https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "cat_inds=[0,1,2]\n",
    "g_inds=[i for i in range(3,775)]\n",
    "c_inds=[i for i in range(775,875)]\n",
    "\n",
    "t_inds=[i for i in range(0,206)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    0       - cp_type\n",
    "    1       - cp_time\n",
    "    2       - cp_dose\n",
    "    3-774   - g-s\n",
    "    775-874 - c-s\n",
    "    \n",
    "    0-401 target\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        #self.conv1=nn.Conv2d(1,10,3)        \n",
    "        #self.conv2=nn.Conv2d(10,10,3)\n",
    "        #self.drop =nn.Dropout2d(p=0.5)\n",
    "        #self.nice =nn.Flatten()\n",
    "        \n",
    "        cat_out=3\n",
    "        g_out=20\n",
    "        c_out=20\n",
    "        g_s=[len(g_inds),400,200,50,50,g_out]\n",
    "        c_s=[len(c_inds),50,50,50,50,c_out]\n",
    "        self.g_layers=[]\n",
    "        for i in range(len(g_s)-1):\n",
    "            self.g_layers.append(nn.Linear(g_s[i],g_s[i+1]))\n",
    "            \n",
    "        self.c_layers=[]\n",
    "        for i in range(len(c_s)-1):\n",
    "            self.c_layers.append(nn.Linear(c_s[i],c_s[i+1]))\n",
    "        \n",
    "        end_dims=[cat_out+g_out+c_out,80,40,len(t_inds)]\n",
    "        self.end_layers=[]\n",
    "\n",
    "        \n",
    "        self.end_layers=[]\n",
    "        for i in range(len(end_dims)-1):\n",
    "            self.end_layers.append(nn.Linear(end_dims[i],end_dims[i+1]))\n",
    "\n",
    "        self.g_layers  =nn.ModuleList(self.g_layers)\n",
    "        self.c_layers  =nn.ModuleList(self.c_layers)\n",
    "        self.end_layers=nn.ModuleList(self.end_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x=self.conv1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.drop(x)\n",
    "        x=self.nice(x)\n",
    "        '''\n",
    "        cat=x[:,cat_inds]\n",
    "        g=x[:,g_inds]\n",
    "        c=x[:,c_inds]\n",
    "        \n",
    "        \n",
    "        for layer in self.g_layers:\n",
    "            g=layer(g)\n",
    "            g=torch.relu(g)\n",
    "            \n",
    "        for layer in self.c_layers:\n",
    "            c=layer(c)\n",
    "            c=torch.relu(c)\n",
    "        \n",
    "        \n",
    "        x=torch.cat([cat,g,c],axis=1)\n",
    "        for layer in self.end_layers[:-1]:\n",
    "            x=layer(x)\n",
    "            x=torch.relu(x)\n",
    "        x=self.end_layers[-1](x)\n",
    "        x=torch.sigmoid(x)*0.999+0.0005\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LSTM_FE(nn.Module):\n",
    "    '''\n",
    "    0       - cp_type\n",
    "    1       - cp_time\n",
    "    2       - cp_dose\n",
    "    3-774   - g-s\n",
    "    775-874 - c-s\n",
    "    \n",
    "    0-401 target\n",
    "    '''\n",
    "    def __init__(self,linear_dims=[],lstm_dims=[]):\n",
    "        super(Net,self).__init__()\n",
    "        #self.conv1=nn.Conv2d(1,10,3)        \n",
    "        #self.conv2=nn.Conv2d(10,10,3)\n",
    "        #self.drop =nn.Dropout2d(p=0.5)\n",
    "        #self.nice =nn.Flatten()\n",
    "        \n",
    "        \n",
    "        self.linear_layers=[]\n",
    "        for i in range(len(linear_dims)-1):\n",
    "            self.linear_layers.append(nn.Linear(linear_dims[i],linear_dims[i+1]))\n",
    "            \n",
    "        self.end_layers=[]\n",
    "\n",
    "        \n",
    "        self.end_layers=[]\n",
    "        for i in range(len(end_dims)-1):\n",
    "            self.end_layers.append(nn.Linear(end_dims[i],end_dims[i+1]))\n",
    "\n",
    "        self.linear_layers  =nn.ModuleList(self.linear_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x=self.conv1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.drop(x)\n",
    "        x=self.nice(x)\n",
    "        '''\n",
    "        cat=x[:,cat_inds]\n",
    "        g=x[:,g_inds]\n",
    "        c=x[:,c_inds]\n",
    "        \n",
    "        \n",
    "        for layer in self.g_layers:\n",
    "            g=layer(g)\n",
    "            g=torch.relu(g)\n",
    "            \n",
    "        for layer in self.c_layers:\n",
    "            c=layer(c)\n",
    "            c=torch.relu(c)\n",
    "        \n",
    "        \n",
    "        x=torch.cat([cat,g,c],axis=1)\n",
    "        for layer in self.end_layers[:-1]:\n",
    "            x=layer(x)\n",
    "            x=torch.relu(x)\n",
    "        x=self.end_layers[-1](x)\n",
    "        x=torch.sigmoid(x)*0.999+0.0005\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    av_loss=0\n",
    "    loading_tqdm=tqdm(enumerate(train_loader),position=0,leave=True,total=len(train_loader))\n",
    "    for i, (data,target,idx) in loading_tqdm:\n",
    "        data,target,idx=data.to(device),target.to(device),idx.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        #loss=F.nll_loss(output,target)\n",
    "        #loss=torch.sum(-torch.log(output)*target-torch.log(1-output)*(1-target),1)/(target.numel())\n",
    "        loss=torch.sum(-torch.log(output)*target-torch.log(1-output)*(1-target))/(target.numel())\n",
    "        #cur_loss=torch.sum(loss).item()\n",
    "        #old_losses=np.copy(loss.detach().numpy())\n",
    "        #old_losses=old_losses/np.sum(old_losses)*np.size(old_losses)\n",
    "        #loss=loss*torch.from_numpy(np.array([training_data.weights_dict[i.item()] for i in idx]).astype(np.float32))\n",
    "        \n",
    "        #for c,cidx in enumerate(idx):\n",
    "        #    training_data.weights_dict[cidx.item()]=training_data.weights_dict[cidx.item()]*old_losses[c]\n",
    "        \n",
    "        #loss=torch.sum(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss=loss.item()\n",
    "        av_loss=i/(i+1)*av_loss+1/(i+1)*cur_loss\n",
    "        loading_tqdm.set_description_str(f\"Epoch:{epoch},Iteration:{i} \")\n",
    "        loading_tqdm.set_postfix_str(f\"Epoch loss: {av_loss},Loss: {cur_loss}\")\n",
    "        #print(f\"Training epoch with {target.shape[0]} samples. Current loss: {cur_loss}. Epoch loss: {av_loss}\")\n",
    "        \n",
    "def validation(model, device, validation_loader, optimizer):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data,target,_=validation_loader.__iter__().__next__()   \n",
    "        data,target=data.to(device),target.to(device)\n",
    "        output=model(data)\n",
    "        loss=torch.sum(-torch.log(output)*target-torch.log(1-output)*(1-target))/(target.numel())\n",
    "        print(f\"Validated {target.shape[0]} samples with a loss of {loss.item()}\")\n",
    "        \n",
    "def test_output(model, device, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data=test_loader.__iter__().__next__()\n",
    "        data.to(device)\n",
    "        output=model(data)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def test_output_save(model, device, test_loader,fname='sub.CSV'):\n",
    "    output=test_output(model,device,test_loader)\n",
    "\n",
    "    sig_id_df=pd.read_csv(root_dir+\"test_features.csv\")['sig_id']\n",
    "    main_df=pd.DataFrame(np.array(output).astype(str),\n",
    "                 columns=pd.read_csv(root_dir+\"train_targets_scored.csv\").columns[1:])\n",
    "    \n",
    "    header=np.array(pd.read_csv(root_dir+\"train_targets_scored.csv\").columns,dtype=str)\n",
    "    header=header.reshape(1,header.shape[0])\n",
    "    \n",
    "    data=np.concatenate((sig_id_df.values.reshape(sig_id_df.array.shape[0],1),main_df.values),axis=1).astype(str)\n",
    "    \n",
    "    whole=np.concatenate([header,data],axis=0)\n",
    "    np.savetxt(fname,whole,fmt=\"%s\",delimiter=',')\n",
    "    return whole\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sig_id', '5-alpha_reductase_inhibitor',\n",
       "        '11-beta-hsd1_inhibitor', ..., 'vitamin_b',\n",
       "        'vitamin_d_receptor_agonist', 'wnt_inhibitor'],\n",
       "       ['id_0004d9e33', '0.00094318355', '0.0016934909', ...,\n",
       "        '0.0017184615', '0.0060428977', '0.00407704'],\n",
       "       ['id_001897cda', '0.00070032815', '0.0005325907', ...,\n",
       "        '0.0006041755', '0.0041928813', '0.0005038914'],\n",
       "       ...,\n",
       "       ['id_ffb710450', '0.0005976581', '0.00086395093', ...,\n",
       "        '0.0005422598', '0.0011519161', '0.0015247206'],\n",
       "       ['id_ffbb869f2', '0.0005612652', '0.0006645365', ...,\n",
       "        '0.00051207165', '0.00075335894', '0.0010716119'],\n",
       "       ['id_ffd5800b6', '0.0007048904', '0.0013147753', ...,\n",
       "        '0.00071444316', '0.0025332312', '0.0023975582']], dtype='<U47')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output_save(model,device,test_loader,fname=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de71290935a0448381d3e6a4659f2119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.023953478783369064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2593b2e57094d2cbd7e97ebe4d48607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.023279456421732903\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f12f8c97904ef3a0333027fa511dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.021333934739232063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d70adf14044e5979193147bbfba56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.01984424516558647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a06577a8eb48df8cb6f26f31215da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.01927703060209751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=Net()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "#optimizer=optim.SGD(model.parameters(),lr=0.001)\n",
    "scheduler=StepLR(optimizer,100,gamma=0.8)\n",
    "\n",
    "\n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "epochs=6\n",
    "for epoch in range(1,epochs+1):\n",
    "    train(model, device, train_full_loader, optimizer, epoch)# for an epoch\n",
    "    validation(model, device, validation_loader, optimizer)# for an epoch\n",
    "    #test(model, device, test_loader)\n",
    "    scheduler.step()#next epoch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.018615854904055595\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "    train(model, device, train_loader, optimizer, epoch)# for an epoch\n",
    "    validation(model, device, validation_loader, optimizer)# for an epoch\n",
    "    #test(model, device, test_loader)\n",
    "    scheduler.step()#next epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.018321024253964424\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "    train(model, device, train_loader, optimizer, epoch)# for an epoch\n",
    "    validation(model, device, validation_loader, optimizer)# for an epoch\n",
    "    #test(model, device, test_loader)\n",
    "    scheduler.step()#next epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc71cb749184b2eaaa0d7e92c0f8ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validated 3814 samples with a loss of 0.018139513209462166\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,1):\n",
    "    train(model, device, train_loader, optimizer, epoch)# for an epoch\n",
    "    validation(model, device, validation_loader, optimizer)# for an epoch\n",
    "    #test(model, device, test_loader)\n",
    "    scheduler.step()#next epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
